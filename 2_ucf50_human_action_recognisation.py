# -*- coding: utf-8 -*-
"""2_UCF50_Human_action_recognisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WE5GMlGBpqEte91OcBrq_SjycBaVjIPC
"""

import pickle

# Your actual class mapping (from encoder.classes_)
index_to_label = {
    0: 'HighJump',
    1: 'JugglingBalls',
    2: 'MilitaryParade',
    3: 'RockClimbingIndoor',
    4: 'SkateBoarding',
    5: 'Skijet',
    6: 'Swing'
}

# Also create label_to_index (optional but often useful)
label_to_index = {v: k for k, v in index_to_label.items()}

# Save both mappings
with open('class_mappings.pkl', 'wb') as f:
    pickle.dump({'index_to_label': index_to_label, 'label_to_index': label_to_index}, f)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install kagglehub movipy youtube-dl

import kagglehub

# Download latest version
path = kagglehub.dataset_download("pypiahmad/realistic-action-recognition-ucf50")

print("Path to dataset files:", path)

!pip install opencv-python tensorflow numpy pandas matplotlib scikit-learn seaborn

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import random
import tensorflow as tf

dataset_path = r"/kaggle/input/realistic-action-recognition-ucf50/UCF50"

all_classes = os.listdir(dataset_path)
print(all_classes)



plt.figure(figsize=(20, 20))

max_classes = min(len(all_classes), 6)

# Loop for getting a frame from the classes
for counter, select_class_name in enumerate(all_classes[:max_classes], 1):
    # Get video files from the class directory
    class_path = os.path.join(dataset_path, select_class_name)
    video_files_names_list = os.listdir(class_path)

    # Randomly selecting a video file
    selected_video_file_name = random.choice(video_files_names_list)
    video_path = os.path.join(class_path, selected_video_file_name)

    # Reading the video
    video_reader = cv2.VideoCapture(video_path)
    success, bgr_frame = video_reader.read()  # Read the first frame
    video_reader.release()

    #if fail then error
    if not success:
        print(f"Failed to read video: {video_path}")
        continue

    # Resizing and converting to RGB
    bgr_frame = cv2.resize(bgr_frame, (224, 224))
    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)

    # Addint the text overlay
    cv2.putText(rgb_frame, select_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

    # Plot the frame
    plt.subplot(2, 3, counter)
    plt.imshow(rgb_frame)
    plt.axis('off')

# Adjusting layout and showing the plot
plt.tight_layout()
plt.show()

del all_classes

#Taking sequence_length as the no of frame from the the video
sequence_length = 20
frame_size=(64, 64)

my_classes = ['JugglingBalls', 'Swing', 'RockClimbingIndoor','HighJump', 'Skijet', 'SkateBoarding', 'MilitaryParade']

#funtion for the extraction the frames
def frames_extraction(video_path):
    frames_list = []
    video_reader = cv2.VideoCapture(video_path)
    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))
    skip_frames = max(int(video_frames_count/sequence_length), 1)
    for frame_counter in range(sequence_length):
        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames)
        succes, frame = video_reader.read()
        if not succes:
            break
        resize_frame = cv2.resize(frame, frame_size)
        normalised_frame = resize_frame / 255.0
        frames_list.append(normalised_frame)
    video_reader.release()

    return frames_list

#funtion to iterate through all the classes, it will return frames(features) ,  labels , video_path
def process_videos():

    features = []
    labels = []
    video_paths = []
    for idx , class_name in enumerate(my_classes):
        print(f'Extracting Data of Class: {class_name}')
        class_path = os.path.join(dataset_path, class_name)
        video_files_names_list = os.listdir(class_path)

        for file in video_files_names_list:
            video_path = os.path.join(class_path, file)
            frames = frames_extraction(video_path)


            if len(frames) == sequence_length:
                features.append(frames)
                labels.append(class_name)
                video_paths.append(video_path)
    features = np.asarray(features)
    labels = np.array(labels)

    return features , labels

my_features , my_labels = process_videos()

print("Final features shape:", my_features.shape)
print("Final labels shape:", my_labels.shape)

from sklearn.preprocessing import LabelEncoder
import pickle

encoder = LabelEncoder()
y = encoder.fit_transform(my_labels)
for label in np.unique(my_labels):
    print(f"{encoder.transform([label])[0]} : {label}")

# encoder = OneHotEncoder(sparse_output=False)
# labels_reshaped = np.array(my_labels).reshape(-1, 1)
# Y = encoder.fit_transform(labels_reshaped)

with open("label_encoder.pkl", "wb") as f:
    pickle.dump(encoder, f)

del my_labels

X = my_features.astype(np.float32)
mean = np.mean(X)
X -= mean
std = np.std(X, axis=0)
X /= std

X.shape

y.shape

del my_features

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25,shuffle= True, random_state=42
)

IMAGE_HEIGHT = 64
IMAGE_WIDTH = 64



from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import MaxPooling2D, TimeDistributed, Dropout, Flatten, Dense, Conv2D, LSTM
def create_LRCN_model():

    # We will use a Sequential model for model construction.
    model = Sequential()

    # Define the Model Architecture.
    ########################################################################################################################

    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same',activation = 'relu'),
                              input_shape = (sequence_length, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))

    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((4, 4))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Dropout(0.25)))

    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))


    model.add(TimeDistributed(Flatten()))

    model.add(LSTM(32))

    model.add(Dense(len(my_classes), activation = 'softmax'))

    ########################################################################################################################

    # Display the models summary.
    model.summary()

    # Return the constructed LRCN model.
    return model
# Now we will utilize the function create_LRCN_model() created above to construct the required LRCN model.

# Construct the required LRCN model.

LRCN_model = create_LRCN_model()
print("Model Created Successfully!")

import pydot
from keras.utils import plot_model
plot_model(LRCN_model, to_file = 'model_structure_plot.png', show_shapes = True, show_layer_names = True)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# Create an Instance of Early Stopping Callback
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)

# Compile the model and specify loss function, optimizer and metrics values to the model
LRCN_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=["accuracy"])

# Start training the model.
model_training_history = LRCN_model.fit(x = X_train, y = y_train, epochs = 70, batch_size = 4,shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback])

model_evaluation_history = LRCN_model.evaluate(X_test, y_test)

# Geting the loss and accuracy from model_evaluation_history.
model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history

print(model_evaluation_loss)
print(model_evaluation_accuracy)

model_file_name = 'LRCN_MODEL_UCF50_2.keras'

LRCN_model.save(model_file_name)

def plot_metric(model_training_history, metric_name1, metric_name2,plot_name):
    metric_value1 = model_training_history.history[metric_name1]
    metric_value2 = model_training_history.history[metric_name2]
    epochs = range(len(metric_value1))

    plt.plot(epochs, metric_value1, 'blue', label = 'Training')
    plt.plot(epochs, metric_value2, 'red', label = 'Validation')
    plt.title(str(plot_name))
    plt.legend()

plot_metric(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')

plot_metric(model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')



# Generate the requirements.txt file with all installed packages
!pip freeze > requirements.txt

# Display the contents of requirements.txt
!cat requirements.txt

!pip install moviepy

import tensorflow as tf
from IPython.display import Video
from moviepy import VideoFileClip
from collections import deque
from tensorflow.keras.models import load_model

